{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d1dccc9",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "activeView": "grid_default",
      "views": {
       "grid_default": {
        "col": 0,
        "height": 3,
        "hidden": false,
        "row": 0,
        "width": 11
       }
      }
     }
    },
    "tags": []
   },
   "source": [
    "# <h1><center><span style='color:darkblue'>Welcome to the Interactive NLP Dashboard for Course Syllabi!</span></center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed08bede-9466-49df-886a-4b5cd930c529",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "activeView": "grid_default",
      "views": {
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/jordan/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jordan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# If an error occurs in this cell, it is likely because you need to pip install a package(s) \n",
    "import pandas as pd\n",
    "import os\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import nltk\n",
    "#nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "#from nltk.stem.lancaster import LancasterStemmer\n",
    "import gensim.corpora as corpora\n",
    "from pprint import pprint \n",
    "import ast\n",
    "from glob import glob\n",
    "from collections import Counter\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import ipywidgets as widgets \n",
    "from ipywidgets import interact, interact_manual, HBox, interactive\n",
    "#from wordcloud import WordCloud \n",
    "import seaborn as sns\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from IPython.display import Markdown, display\n",
    "import nltk\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "def printmd(string):\n",
    "    display(Markdown(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cce88a39-c01c-473e-a02f-08f884c80458",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "activeView": "grid_default",
      "views": {
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "# Set directory and get data - may need to modify this to wherever you have the data stored \n",
    "#os.chdir(\"/Users/jordan/Desktop/NLP_Syllabi_Project_copy/data_and_output\")\n",
    "data_orig = pd.read_csv(\"cleaned_data_new.csv\", lineterminator='\\n')\n",
    "data_orig.drop(data_orig.columns[[0, 1, 2]], axis = 1, inplace = True) #drop meaningless indice columns \n",
    "data_orig['index'] = data_orig.index\n",
    "# Create new data, just text and index \n",
    "map_names = data_orig[['title', 'File_name', 'index']] \n",
    "#print(data_orig.head())\n",
    "#len(data_orig.index) #4889 documents \n",
    "data = data_orig[['Corpus', 'index']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9126289-991a-4503-a78b-e9b0c73f9ab2",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "activeView": "grid_default",
      "views": {
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "# This is where we set up the input approach (full syllabi vs. reduced syllabi (course descriptions) vs. bibliographic content)\n",
    "\n",
    "# Further preprocessing (removes stop words and also words 3 characters or less\n",
    "# and also non-english words) \n",
    "# Note: There was additional preprocessing done in previous steps (earlier scripts located in scripts folder)\n",
    "words = set(nltk.corpus.words.words())\n",
    "words.update([\"africana\"])\n",
    "stop_words = stopwords.words('english')\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in text.split():\n",
    "        if token not in stop_words and len(token) >3 and token in words:\n",
    "            result.append(token)\n",
    "    return result\n",
    "\n",
    "#doc_sample = data[data['index'] == 1].values[0][0]\n",
    "#print('original document: ')\n",
    "#words = []\n",
    "#for word in doc_sample.split(' '):\n",
    "#    words.append(word)\n",
    "#print(words)\n",
    "#print('\\n\\n tokenized and document: ')\n",
    "#print(preprocess(doc_sample))\n",
    "\n",
    "processed_text = data['Corpus'].astype('str').map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c3693dd-fd04-4b56-a95c-30c0ab1b7ef2",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "activeView": "grid_default",
      "views": {
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "# Create word lists for exploratory data analysis \n",
    "\n",
    "# Get all data row lists into one list \n",
    "all_words_list=[]\n",
    "for index, row in processed_text.items():\n",
    "    contri = row\n",
    "    all_words_list.extend(contri)\n",
    "\n",
    "# Get all data row lists with unique elements into one list (to see individual words by documents - removes duplicates within a single document)\n",
    "all_docs_list=[]\n",
    "for index, row in processed_text.items():\n",
    "    contri = list(set(row))\n",
    "    all_docs_list.extend(contri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237ea771",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "activeView": "grid_default",
      "views": {
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    }
   },
   "source": [
    "## <span style='color:darkblue'>Word Frequency Table</span>\n",
    "\n",
    "**README:** The <mark style=\"background-color: lightblue\">Number of Words</mark> parameter will control the number of words you want a frequency table for in descending order of frequency; for example, if you choose 25 for this parameter, then you will get a frequency table for the 25 most frequent words (in the order of 1-25, with 1 being most frequent)\n",
    "\n",
    "**NOTE:** <span style='color:red'> Output is truncated for tables with more than 10k words </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f59413dc-2553-4279-bc20-a43626e0b462",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "activeView": "grid_default",
      "views": {
       "grid_default": {
        "col": 0,
        "height": 5,
        "hidden": false,
        "row": 3,
        "width": 6
       }
      }
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25a51b78e0d84de1bd6bd528a9d4d016",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=11181, description='num', max=22369, min=1, step=10), Output()), _dom_clâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Explore word frequencies - this shows the proportion of times a word appears across all 4889 syllabi documents \n",
    "pd.set_option('display.max_rows', 10000) # change this as needed\n",
    "\n",
    "def word_interacter(num=(1,22369,10)):\n",
    "    y = Counter(all_docs_list)\n",
    "    freq = y.most_common(num)\n",
    "    df = pd.DataFrame(freq, columns=[\"Word\", \"Proportion\"])\n",
    "    df['Proportion'] = df['Proportion'].div(4889).round(2) \n",
    "    #df['Percentage'] = (df['Percentage']*100).astype('str').str.strip(\".0\") + \"%\"\n",
    "    return df\n",
    "uit = interact(word_interacter, num=(1, 22369, 10))\n",
    "uit.widget.children[0].style= {'description_width': 'initial'}\n",
    "uit.widget.children[0].description = \"Number of Words\" \n",
    "# To save\n",
    "#df.to_csv(\"word_freq.csv\")\n",
    "# NOTE: Add a filter by proportion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d6c9f5b-916d-4d10-9216-386413c5d0bf",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "activeView": "grid_default",
      "views": {
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "## Word Cloud of Most Common Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d8d5784-b4ed-4971-924e-ce4ef11960ef",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "activeView": "grid_default",
      "views": {
       "grid_default": {
        "col": 7,
        "height": 5,
        "hidden": true,
        "row": 3,
        "width": 5
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "# Create word cloud to see a visual of most common words \n",
    "# Didn't make this interactive because it takes too long to load and refresh \n",
    "#long_string = ' '.join(all_words_list)\n",
    "#wcloud = WordCloud(background_color=\"white\", max_words = 5000, \n",
    "#                         contour_width = 3, contour_color='steelblue')\n",
    "#wcloud.generate(long_string)\n",
    "#wcloud.to_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8beb3d00",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "activeView": "grid_default",
      "views": {
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    }
   },
   "source": [
    "## <span style='color:darkblue'>Histogram of Word Frequencies</span>\n",
    "\n",
    "**README:** The <mark style=\"background-color: lightblue\">Number of Words</mark> parameter will control the number of words you want a histogram of word frequencies for in descending order of frequency; so if you choose 25 for this parameter, then you will get a histogram for the 25 most frequent words (in the order of 1-25, with 1 being most frequent). In addition, the <mark style=\"background-color: lightblue\">Bar Size</mark> parameter will control the appearance of the histogram, specifically it changes the **size** of the bars so that the chart can be more easily viewed. With less words (i.e., when you make the Number of Words parameter smaller), you will likely need to decrease the size of the bars; to do this, you will scroll the Bar Size parameter **to the left**. To zoom in and make the bars bigger, scroll the Bar Size parameter **to the right**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b12a4a8-66df-4dc0-8d82-25703dfcf7b1",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "activeView": "grid_default",
      "views": {
       "grid_default": {
        "col": 6,
        "height": 8,
        "hidden": true,
        "row": 8,
        "width": 6
       }
      }
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efa929c913da4482aef70393442655c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=50, description='Num_words', max=1000, min=1), IntSlider(value=20, descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Histogram of words (gives up to 1000 most common words)\n",
    "%matplotlib inline\n",
    "def histo_fun(Num_words=50, Zoom_image=20):\n",
    "    sns.set(rc={\"figure.figsize\": (8, Zoom_image)})\n",
    "    counter=Counter(all_words_list)\n",
    "    most=counter.most_common(Num_words)\n",
    "    x, y= [], []\n",
    "    for word,count in most[:]:\n",
    "        x.append(word)\n",
    "        y.append(count)\n",
    "    p = sns.barplot(x=y,y=x) \n",
    "    p.set_xlabel('Word Count')    \n",
    "    p.xaxis.set_label_position('top') \n",
    "    p.xaxis.labelpad = 20\n",
    "    p.xaxis.tick_top()\n",
    "    plt.show()\n",
    "    #return p \n",
    "uih = interact(histo_fun, Num_words=(1, 1000, 1), Zoom_image=(4, 500, 1))\n",
    "uih.widget.children[0].style= {'description_width': 'initial'}\n",
    "uih.widget.children[0].description = \"Number of Words\" \n",
    "uih.widget.children[1].style= {'description_width': 'initial'}\n",
    "uih.widget.children[1].description = \"Bar Size\"\n",
    "# Put x-axis on top \n",
    "# Also provide histogram of flip (least frequent)\n",
    "# Add more sliders and make them more descriptive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b922577-58f2-4ada-929d-45393f491482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary\n",
    "#dictionary = gensim.corpora.Dictionary(processed_text_new)\n",
    "#count = 0 \n",
    "#for k, v in dictionary.iteritems():\n",
    "#    print(k, v)\n",
    "#    count += 1\n",
    "#    if count > 10:\n",
    "#        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5531a9",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "activeView": "grid_default",
      "views": {
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    }
   },
   "source": [
    "## <span style='color:darkblue'>Topic Modeling via Latent Dirichlet Allocation (Top 10 Words Per Topic)</span> \n",
    "\n",
    "This is a three-step process that allows the user quite a bit of flexibility over how they analyze the data to come up with topics. Please follow the outlined steps and use the readme descriptions below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fb9e54-2cd9-49f9-973e-e1a543fde43c",
   "metadata": {},
   "source": [
    "### Step 1: Selecting Your Data\n",
    "\n",
    "**README:** The 'Data' drop-down tab allows you to select the type of syllabi data you wish to work with. The two options are the full syllabi content (i.e., everything contained within syllabi) or just the course descriptions content. The default option is the full syllabi content. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bb7deb9-6776-433b-9a4f-be2b92fc039f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfa8eee1450244608d86a5325e483bcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Data', options=('Full Syllabi', 'Course Descriptions Only'), valueâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create option for text selection (Full Syllabi vs Reduced Syllabi vs. Bibliographic content (add that last one later))\n",
    "def data_grab(column=\"Full Syllabi\"):\n",
    "    global processed_text\n",
    "    global counter \n",
    "    if column==\"Full Syllabi\":\n",
    "        data = data_orig[['Corpus', 'index']]\n",
    "        processed_text = data['Corpus'].astype('str').map(preprocess)\n",
    "        counter = \"full syllabi\"\n",
    "        printmd(\"You are now analyzing the full syllabi content\") \n",
    "    if column==\"Course Descriptions Only\":\n",
    "        data = data_orig[['description', 'index']] \n",
    "        data = data.rename(columns = {\"description\" : \"Corpus\"})\n",
    "        processed_text = data['Corpus'].astype('str').map(preprocess)\n",
    "        counter = \"course description\"\n",
    "        printmd(\"You are now analyzing just course description content\") \n",
    "# Interactive piece \n",
    "ui_1 = interact(data_grab, column=widgets.Dropdown(options=[\"Full Syllabi\",\"Course Descriptions Only\"], description=\"Data\")) \n",
    "#print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de07550f-b69e-4941-94cb-5eea1343a5fe",
   "metadata": {},
   "source": [
    "### Step 2: Selecting an Analytic Approach (Bag-of-Words vs. N-grams) \n",
    "\n",
    "**README:** The 'Options' drop-down box allows you to select from three options: Bag-of-Words, Bi-grams, and Tri-grams. Once you have selected an approach, hit the 'Apply Approach' button to apply this change. Note that you will need to select and apply an approach before you can run the topic model in Step 3. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ad74bdc-66ab-4f4c-a62b-4cb129572f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c0737093f1b4b18af57d985b3c8137b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Options', options=('Bag-of-Words', 'Bi-gram', 'Tri-gram'), value='â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Here we give options for different modeling approaches (BoW vs. N-grams)\n",
    "\n",
    "# Main function that is interactive \n",
    "def data_chooser(approach=\"Bag-of-Words\"):\n",
    "    global processed_text_new\n",
    "    global counter2\n",
    "    def make_bigrams(texts):\n",
    "        return [bigram_mod[itm] for itm in texts]\n",
    "    def make_trigrams(texts):\n",
    "        return [trigram_mod[bigram_mod[itm]] for itm in texts]\n",
    "    if approach==\"Bag-of-Words\":\n",
    "        processed_text_new = processed_text\n",
    "        counter2 = \"Bag-of-Words model\"\n",
    "        printmd(\"You are now working with the Bag-of-Words data\")\n",
    "        #return processed_text_new\n",
    "    if approach==\"Bi-gram\":\n",
    "        bigram = gensim.models.Phrases(processed_text, min_count=20)\n",
    "        bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "        processed_text_new = make_bigrams(processed_text)\n",
    "        processed_text_new = pd.Series(processed_text_new)\n",
    "        counter2 = \"bi-gram model\"\n",
    "        printmd(\"You are now working with bi-gram data\")\n",
    "        #return processed_text_new \n",
    "    if approach==\"Tri-gram\":\n",
    "        bigram = gensim.models.Phrases(processed_text, min_count=20)\n",
    "        trigram = gensim.models.Phrases(bigram[processed_text], threshold=20)\n",
    "        # Faster way to get a sentence clubbed as a trigram/bigram\n",
    "        bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "        trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "        processed_text_new = make_trigrams(processed_text)\n",
    "        processed_text_new = pd.Series(processed_text_new)\n",
    "        counter2 = \"tri-gram model\"\n",
    "        printmd(\"You are now working with tri-gram data\")\n",
    "        #return processed_text_new \n",
    "# Interactive piece \n",
    "ui_2 = interact_manual(data_chooser, approach=widgets.Dropdown(options=[\"Bag-of-Words\",\"Bi-gram\",\"Tri-gram\"], description=\"Options\"))\n",
    "ui_2.widget.children[1].style= {'description_width': 'initial'}\n",
    "ui_2.widget.children[1].description = \"Apply Approach\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4eff4df-d20f-4860-b1a1-5836cdccb5fc",
   "metadata": {},
   "source": [
    "### Step 3: Running the Topic Model \n",
    "\n",
    "**README:** The <mark style=\"background-color: lightblue\">Lower Exclusion Filter</mark> parameter controls what words you want to exclude from the topic modeling analysis based on the number of documents words appear in (and it is a less than exclusion). For example, if you set this parameter to 25, then words appearing in less than 25 documents will be excluded from the analaysis (the idea is that you do want to exclude words that are too rare and won't contribute much to analysis). In addition, the <mark style=\"background-color: lightblue\">Upper Exclusion Filter</mark> parameter will control what words you want to exclude from the topic modeling analysis based on the proportion of documents words appear in (and is a more than exclusion). For example, if you set this parameter to 0.35, then words appearing in more than 35% of the documents will be excluded from the analysis (the idea is that you do want to exclude words that are too frequent and thus may be common words that aren't too informative for our purposes but could dominate the analysis due to their frequency). Lastly, the <mark style=\"background-color: lightblue\">Topic Number</mark> parameter controls the number of topics that you want the topic model to produce and output. \n",
    "\n",
    "**NOTE:** <span style='color:red'> Changing model parameters will produce new output and may take awhile to refresh.  </span>\n",
    "    \n",
    "**IMPORTANT:** <span style='color:red'> The specifications output reflects the options selected in Steps 1 and 2 </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "047c7454-b4e3-46d1-a58d-316b064b74e5",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "activeView": "grid_default",
      "views": {
       "grid_default": {
        "col": 0,
        "height": 28,
        "hidden": false,
        "row": 8,
        "width": 12
       }
      }
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec559a1f5978438ba94f58b4b9393db9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=25, description='low_freq', max=50, min=1), FloatSlider(value=0.35000000â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Interactive LDA modeling \n",
    "import threading\n",
    "from IPython.display import display\n",
    "import time\n",
    "\n",
    "# function \n",
    "def output(low_freq, high_freq, topic_num):\n",
    "    #time.sleep(3)\n",
    "    progress = widgets.FloatProgress(value=0.0, min=0.0, max=1.0, description = \"Loading:\")\n",
    "    finished = False\n",
    "    def work(progress): \n",
    "        total = 200\n",
    "        for i in range(total):\n",
    "            if finished != True:\n",
    "                time.sleep(0.2)\n",
    "                progress.value = float(i+1)/total\n",
    "            else:\n",
    "                progress.value = 200\n",
    "                progress.description = \"Completed:\"\n",
    "                break\n",
    "    try: \n",
    "        thread = threading.Thread(target=work, args=(progress,))\n",
    "        display(progress)\n",
    "        thread.start()\n",
    "        dictionary = gensim.corpora.Dictionary(processed_text_new)\n",
    "        dictionary.filter_extremes(no_below=low_freq, no_above = high_freq) #modify these \n",
    "        bow_corpus = [dictionary.doc2bow(doc) for doc in processed_text_new]\n",
    "        lda_model = gensim.models.LdaModel(bow_corpus, num_topics=topic_num, id2word=dictionary) \n",
    "        all_topics = lda_model.print_topics(topic_num)\n",
    "        top_words_per_topic = []\n",
    "        for t in range(lda_model.num_topics):\n",
    "            y = [str(t)]\n",
    "            x = lda_model.get_topic_terms(t,10)\n",
    "            y.extend([dictionary[pair[0]] for pair in x])\n",
    "            top_words_per_topic.append(y)\n",
    "            y=[]\n",
    "        top_words_per_topic\n",
    "        word_list = [\"Word\" + str(i) for i in list(range(1,11))]\n",
    "        word_list\n",
    "        word_list.insert(0, \"Topic\")\n",
    "        out = pd.DataFrame(top_words_per_topic, columns=word_list)\n",
    "        #out.insert(0, \"Topic\")\n",
    "        #out = out.set_index('Topic')\n",
    "        docs_per_topic = [[] for _ in all_topics]\n",
    "        for doc_id, doc_bow in enumerate(bow_corpus):\n",
    "            doc_topics = lda_model.get_document_topics(doc_bow)\n",
    "            for topic_id, score in doc_topics:\n",
    "                docs_per_topic[topic_id].append((doc_id, score))\n",
    "        for doc_list in docs_per_topic:\n",
    "            doc_list.sort(key=lambda id_and_score: id_and_score[1], reverse=True)\n",
    "        docs_per_topic = [item[:10] for item in docs_per_topic]\n",
    "        data_list = []\n",
    "        for lst in docs_per_topic:\n",
    "            x = pd.DataFrame(lst, columns=[\"index\", \"probability\"])\n",
    "            y = pd.merge(x, map_names, how='left', on='index')\n",
    "            y.drop(y.columns[[0,1]], axis=1, inplace=True)\n",
    "            y = pd.DataFrame(y.stack())\n",
    "            y = y.transpose()\n",
    "            y = pd.DataFrame(y.values)\n",
    "            data_list.append(y)\n",
    "        df = pd.concat(data_list)\n",
    "        df.columns=[\"syllabus 1\", \"filename 1\", \"syllabus 2\", \"filename 2\", \"syllabus 3\", \"filename 3\", \"syllabus 4\", \n",
    "                                     \"filename 4\", \"syllabus 5\", \"filename 5\", \"syllabus 6\", \"filename 6\", \"syllabus 7\", \"filename 7\",\n",
    "                                     \"syllabus 8\", \"filename 8\", \"syllabus 9\", \"filename 9\", \"syllabus 10\", \"filename 10\"]\n",
    "        numbers = list(range(0,topic_num))\n",
    "        numbers = [y for x in numbers for y in (x,)*1]\n",
    "        df.insert(0, \"Topic\", numbers)\n",
    "        out = out.rename_axis(\"Topic\", axis=\"columns\")\n",
    "        out.drop(\"Topic\", axis=1, inplace=True)\n",
    "        df = df.reset_index(drop=True)\n",
    "        df = df.rename_axis(\"Topic\", axis=\"columns\")\n",
    "        df.drop(\"Topic\", axis=1, inplace=True)\n",
    "        # Coherence \n",
    "        cm = CoherenceModel(model=lda_model, corpus=bow_corpus, coherence='u_mass')\n",
    "        coherence = round(cm.get_coherence(), 2) \n",
    "        #df = df.set_index('Topic')\n",
    "        #syllabi = pd.DataFrame([t for lst in docs_per_topic for t in lst], columns=['index', 'probability'])\n",
    "        #numbers = list(range(0,topic_num))\n",
    "        #numbers = [y for x in numbers for y in (x,)*1]\n",
    "        #syllabi.insert(0, \"Topic\", numbers)\n",
    "        #syllabi = pd.merge(syllabi, map_names, how='left', on='index')\n",
    "        #syllabi.drop(syllabi.columns[[1, 2]], axis = 1, inplace = True)\n",
    "        #syllabi = syllabi.rename(columns={'title': 'Associated Syllabus'})\n",
    "        #out['Topic']=out['Topic'].astype(int)\n",
    "        #syllabi['Topic']=syllabi['Topic'].astype(int)\n",
    "        #newout = pd.merge(out, syllabi, how='left', on='Topic')\n",
    "        finished = True\n",
    "        printmd(\"**SPECIFICATIONS:** You are analyzing the \" + counter + \" content with a \" + counter2)\n",
    "        print(\" \")\n",
    "        printmd(\"**Model Coherence:** \" + str(coherence) + \" (the higher the value, the better the model fit)\")\n",
    "        display(out) # use this to return other output\n",
    "        print('')\n",
    "        printmd(\"Now the top ten most associated syllabi for each topic with file names...\")\n",
    "        return df \n",
    "    except NameError:\n",
    "        finished = True\n",
    "        printmd(\"**<span style='color:red'>ERROR! You have not selected a modeling approach!</span>**\")\n",
    "        printmd(\"**<span style='color: red'>Please go back and apply a modeling approach in Step 2</span>**\")\n",
    "# Make it interactive \n",
    "ui_3 = interact_manual(output,low_freq=(1, 50, 1), high_freq=(0.05, 0.7, 0.05), topic_num = (10, 100, 10))\n",
    "ui_3.widget.children[0].style= {'description_width': 'initial'}\n",
    "ui_3.widget.children[0].description = \"Lower Exclusion Filter\" \n",
    "ui_3.widget.children[1].style= {'description_width': 'initial'}\n",
    "ui_3.widget.children[1].description = \"Upper Exclusion Filter\"\n",
    "ui_3.widget.children[2].style= {'description_width': 'initial'}\n",
    "ui_3.widget.children[2].description = \"Topic Number\"\n",
    "ui_3.widget.children[3].style= {'description_width': 'initial'}\n",
    "ui_3.widget.children[3].description = \"Run Model\"\n",
    "\n",
    "# NOTES FOR UPDATING THIS: \n",
    "# Make threshold displays consistent \n",
    "# Give some kind of loading bar \n",
    "# IMPORTANT: Provide list of documents (course topics/number/title; link to syllabus) that topics are associated with \n",
    "# Upload drive links into dataframe \n",
    "# Print out the parameters for ease of interpretation \n",
    "# See if there is an export exact settings option/capability \n",
    "# Add in more descriptions for the sliders and graphs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1c142d-b1e1-4633-86d2-791b2d67793c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1209a414-9b8c-4085-8eb9-9ff07392870a",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "activeView": "grid_default",
      "views": {
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "extensions": {
   "jupyter_dashboards": {
    "activeView": "grid_default",
    "version": 1,
    "views": {
     "grid_default": {
      "cellMargin": 2,
      "defaultCellHeight": 60,
      "maxColumns": 12,
      "name": "grid",
      "type": "grid"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
